# TP AIOps : Pipeline RAG Continu avec D√©ploiement Blue/Green

**Dur√©e :** 1h30
**Niveau :** Avanc√©
**Objectif :** Impl√©menter une architecture AIOps r√©siliente capable de mettre √† jour sa base de connaissance en temps r√©el sans interruption de service (Zero Downtime).

---

## 1. Introduction et Architecture

Dans ce TP, nous n'allons pas simplement cr√©er un chatbot. Nous allons cr√©er un **syst√®me vivant**.
L'objectif est de r√©soudre un probl√®me classique en production : **Comment mettre √† jour les donn√©es d'une IA sans √©teindre le serveur ?**

Nous utiliserons une architecture **Blue/Green** locale orchestr√©e par Docker :

*   **üìÇ Data Layer** : Un dossier partag√© contenant vos fichiers de connaissances (`.md`).
*   **üëÄ Builder (Watchdog)** : Un service autonome qui surveille ce dossier. D√®s qu'un fichier est modifi√©, il :
    1.  Reconstruit l'index vectoriel (FAISS) avec les embeddings Gemini.
    2.  Sauvegarde le nouvel index dans un dossier versionn√© (ex: `v2`).
    3.  Appelle le **Router** pour basculer le trafic.
*   **üîÄ Router** : Un reverse-proxy intelligent. Il sait quelle version (v1 ou v2) est active et redirige le trafic utilisateur vers le bon backend.
*   **üß† Backend (v1 & v2)** : Deux conteneurs identiques qui tournent en parall√®le. Ils chargent l'index qu'on leur assigne.
*   **üíª UI** : Une interface simple (Streamlit) pour tester le chat.

---

## 2. Pr√©paration de l'environnement

### Structure du projet
Cr√©ez l'arborescence suivante :

```
aiops-tp/
‚îú‚îÄ‚îÄ .env                # Vos secrets (API Key)
‚îú‚îÄ‚îÄ docker-compose.yml  # Orchestration
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ knowledge/      # Vos fichiers .md
‚îú‚îÄ‚îÄ backend/            # API RAG (FastAPI)
‚îú‚îÄ‚îÄ builder/            # Service de build auto
‚îú‚îÄ‚îÄ router/             # Load balancer intelligent
‚îî‚îÄ‚îÄ ui/                 # Interface utilisateur
```

### Configuration (.env)
Cr√©ez un fichier `.env` √† la racine :

```ini
GEMINI_API_KEY=votre_cle_gemini_ici
GEMINI_EMBED_URL=https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent
GEMINI_CHAT_URL=https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent
ROUTER_PORT=8000
BACKEND_PORT_V1=8101
BACKEND_PORT_V2=8102
DATA_DIR=/data
INDICES_DIR=/data/indices
```

---

## 3. Impl√©mentation des Services

### A. Le Backend RAG (`backend/`)
C'est le cerveau. Il re√ßoit une question, cherche dans son index FAISS local, et demande √† Gemini de r√©pondre.
*   **Fichier cl√©** : `app.py`
*   Il doit charger l'index situ√© dans `INDEX_PATH` au d√©marrage.

### B. Le Router (`router/`)
C'est l'aiguilleur du ciel.
*   **Fichier cl√©** : `router.py`
*   Il lit un fichier partag√© `active_version` (qui contient "v1" ou "v2").
*   Si "v1" est actif, il redirige vers le conteneur `rag-backend-v1`.
*   Il expose un endpoint `/activate?version=v2` que le Builder appellera.

### C. Le Builder / Watchdog (`builder/`)
C'est l'ouvrier automatis√©.
*   **Fichier cl√©** : `builder.py`
*   Utilise la librairie `watchdog` pour √©couter les √©v√©nements fichiers sur `/data/knowledge`.
*   Au moindre changement :
    1.  Il lit tous les fichiers.
    2.  Il g√©n√®re les embeddings via l'API Gemini.
    3.  Il √©crit l'index FAISS dans le dossier de la version inactive (ex: `indices/v2`).
    4.  Il appelle `POST http://router:8000/activate` pour basculer le trafic.

### D. L'Interface UI (`ui/`)
Simple client Streamlit.
*   Appelle uniquement le Router. Elle ne sait pas qu'il y a deux backends derri√®re.

---

## 4. Orchestration (Docker Compose)

Le fichier `docker-compose.yml` lie tout ensemble.
Points d'attention :
*   Les volumes partag√©s : `router`, `builder` et `backend` doivent tous voir `/data` et `/indices`.
*   Les variables d'environnement : Chaque backend doit savoir s'il est v1 ou v2 (via `INDEX_PATH`).

---

## 5. D√©roulement du TP (Sc√©nario de Test)

### √âtape 1 : D√©marrage
Lancez la stack :
```bash
docker-compose up --build -d
```
V√©rifiez que les 5 conteneurs sont "Up" (`docker-compose ps`).

### √âtape 2 : Initialisation des donn√©es
Ajoutez un premier fichier de connaissance dans `data/knowledge/intro.md` :
```markdown
# AIOps
AIOps stands for Artificial Intelligence for IT Operations.
```
*Observez les logs du builder* : il doit d√©tecter le fichier et construire l'index v1.

### √âtape 3 : Premier Test
Allez sur `http://localhost:8501`.
Demandez : "What is AIOps?".
Le syst√®me doit r√©pondre avec la d√©finition ci-dessus.

### √âtape 4 : La Mise √† Jour "AIOps" (Le c≈ìur du sujet)
C'est le moment de v√©rit√©. Nous allons simuler une mise √† jour de prod.

1.  **Sans arr√™ter les conteneurs**, modifiez le fichier `data/knowledge/intro.md` (ou cr√©ez-en un nouveau).
2.  Ajoutez une information cruciale, par exemple :
    > "IMPORTANT: The support hotline for AIOps is 555-0199."
3.  Sauvegardez le fichier.

### √âtape 5 : V√©rification Automatique
Regardez imm√©diatement les logs du builder :
```bash
docker-compose logs -f builder
```
Vous devriez voir :
> Detected change -> rebuilding index...
> Activated v2

Retournez sur l'UI et posez la question : "What is the support hotline?".
**Si le syst√®me vous r√©pond "555-0199", vous avez r√©ussi.**
Vous avez mis √† jour la connaissance d'une IA en production sans aucune interruption de service.

---

## 6. Pour aller plus loin (Bonus)
*   Ajouter un endpoint `/health` au Router qui v√©rifie la sant√© du backend actif.
*   Ajouter Prometheus pour monitorer le temps de r√©ponse des embeddings.
